{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 4: Transfer Learning and BERT!\n",
    "\n",
    "11/20/2023, Ankit Ramakrishnan, Harshitha Somala, Nidhi Bodar, Felix Muzny\n",
    "\n",
    "Due on: 11/21/2023 @ 9pm\n",
    "\n",
    "Agenda\n",
    "------\n",
    "+ Get an overview of the BERT architecture\n",
    "+ Download and play with a pre-trained BERT model\n",
    "+ Train your own network that has a pre-trained BERT component\n",
    "+ Play with some parameters!\n",
    "\n",
    "\n",
    "Summary\n",
    "----\n",
    "This lab will guide you through the setup and uses of pretrained models in `Tensorflow` and `transformers`. In this lab we focus on BERT (https://github.com/google-research/bert/) an __encoder only transformer model__. We will use the pretrained model to extract features from text and use these features to train a classifier to classify different types of data. We will setup a binary classifier on the IMDB dataset. You will be working on training new models on the other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 0: Who is in your group?\n",
    "\n",
    "Please work in groups of up to 3 people!\n",
    "\n",
    "__Josef LaFranchise__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TASK 1: BERT questions\n",
    "---\n",
    "\n",
    "Before we dive into the code, we'd like you to take a moment to familiarize yourself with the architecture of BERT.\n",
    "\n",
    "Take a look at the following resources as a group:\n",
    "- [Illustrated BERT](http://jalammar.github.io/illustrated-bert/)\n",
    "- SLP Chapter 11.1\n",
    "- [BERT paper](https://aclanthology.org/N19-1423/)\n",
    "\n",
    "Answer the questions in the next cell about the BERT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. What does BERT stand for? __Bidirectional Encoder Representations from Transformers__\n",
    "2. What task is the BERT pre-trained model trained on? __It was trained on the task of predicting the masked word in a sequence using a large amount of text.__\n",
    "3. How many parameters does BERT_{BASE} have? BERT_{LARGE}?   __The base model has 110 million parameters, while the large model has 340 million parameters.__\n",
    "4. What is the architecture of BERT_{BASE}? (what kind of neural network is it, what are the relevant dimensionality numbers?)  __The base model is a stack of 24 transformers with 768 hidden units and 12 attention heads.__\n",
    "5. What can we feed as input to BERT?  __An arbitrary sequence of tokens that starts with the CLS token.__\n",
    "6. What is the output of BERT?  __A sequence of vectors for each of the input positions.__\n",
    "7. If we want to use BERT to help with a classification task, what is a high-level description of what we'll do? (2 - 3 sentences.)  __We will only consider the vector produced by the CLS token as output. We will supply this as an input to a neural network that will provide the value of the classification.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Installation Intermission\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1C-vVvT3cEj",
    "outputId": "d5f4c5b8-af8b-49aa-8bc0-8d5ff57c1f81",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.35.0\n",
      "  Using cached transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "Requirement already satisfied: tensorflow==2.14.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (2.14.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from transformers==4.35.0) (4.64.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-macosx_10_7_x86_64.whl (441 kB)\n",
      "\u001b[K     |████████████████████████████████| 441 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from transformers==4.35.0) (2.27.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from transformers==4.35.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from transformers==4.35.0) (2022.3.15)\n",
      "Requirement already satisfied: filelock in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from transformers==4.35.0) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from transformers==4.35.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from transformers==4.35.0) (1.22.4)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "  Downloading tokenizers-0.14.1-cp39-cp39-macosx_10_7_x86_64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (4.24.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (0.34.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.12.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (0.5.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.59.0)\n",
      "Requirement already satisfied: setuptools in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (61.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.0.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.14.1)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.16.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.14.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (16.0.6)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (23.5.26)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.14.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (3.6.0)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.26.2-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 469 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (4.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.37.1)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
      "\u001b[K     |████████████████████████████████| 168 kB 704 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.35.0) (3.0.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.23.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.35.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.35.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.35.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.35.0) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/shreyasudhanva/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Installing collected packages: fsspec, numpy, huggingface-hub, tokenizers, safetensors, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.2.0\n",
      "    Uninstalling fsspec-2022.2.0:\n",
      "      Successfully uninstalled fsspec-2022.2.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.26.2 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.2 which is incompatible.\u001b[0m\n",
      "Successfully installed fsspec-2023.12.1 huggingface-hub-0.17.3 numpy-1.26.2 safetensors-0.4.1 tokenizers-0.14.1 transformers-4.35.0\n"
     ]
    }
   ],
   "source": [
    "# make sure that the correct version of transformers and tensorflow is installed\n",
    "# https://huggingface.co/docs/transformers/index\n",
    "# https://www.tensorflow.org/install/pip\n",
    "!pip install transformers==4.35.0 tensorflow==2.14.0\n",
    "\n",
    "# if you are on a mac w/ an M1 chip, you'll need a specific tensorflow-metal version\n",
    "# !pip install tensorflow-metal==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "70O-tnuj3U72",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if you want, not required\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPgW_W1i3U73",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ensure that your versions match the ones below. If you are running this locally, install the packages with :\n",
    "\n",
    "```bash\n",
    "pip install transformers==4.35.0 tensorflow==2.14.0\n",
    "```\n",
    "\n",
    "If you get an error on import like:\n",
    "\n",
    "```\n",
    "NotFoundError: dlopen(ENVIRONMENT PATH/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '__ZN10tensorflow8internal10LogMessage16VmoduleActivatedEPKci'\n",
    "```\n",
    "\n",
    "This means that you are on a mac and you don't have the right `tensorflow-metal` [version installed](https://pypi.org/project/tensorflow-metal/):\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install tensorflow-metal==1.1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc5ROhEU3U73",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check your versions with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzyFio-l3U73",
    "outputId": "d263f3e3-4e5b-4f1a-f328-2234296c29ac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensforflow Version :  2.14.0\n",
      "Transformers Version :  4.31.0\n"
     ]
    }
   ],
   "source": [
    "# Version Info\n",
    "print(\"Tensforflow Version : \" ,tf.__version__)\n",
    "print(\"Transformers Version : \" ,transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPd9hZXtC1RB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TASK 2: The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3R6TcHg3U74",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will be using reviews from the UCI Machine Learning Repository\n",
    "\n",
    "URL : https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "This should have three files :\n",
    "\n",
    "1. imbd_labelled.txt\n",
    "2. amazon_cells_labelled.txt\n",
    "3. yelp_labelled.txt\n",
    "\n",
    "We use the tensorflow utils to download and extract the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUrzd6_x3U74",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a peek at the data with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "X9vOov4r3U74",
    "outputId": "5d5f1162-1126-44bc-ce1f-14237cc10596",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       text  sentiment\n",
      "1.0       Try talking with ChatGPT, our new AI system wh...          1\n",
      "3.0       THRILLED to share that ChatGPT, our new model ...          1\n",
      "4.0       As of 2 minutes ago, @OpenAI released their ne...          0\n",
      "5.0       Just launched ChatGPT, our new AI system which...          1\n",
      "6.0       As of 2 minutes ago, @OpenAI released their ne...          0\n",
      "...                                                     ...        ...\n",
      "219287.0  One of my new favorite thing to do with #ChatG...          1\n",
      "219289.0  Other Software Projects Are Now Trying to Repl...          0\n",
      "219290.0  I asked #ChatGPT to write a #NYE Joke for SEOs...          1\n",
      "219291.0  chatgpt is being disassembled until it can onl...          0\n",
      "219292.0  2023 predictions by #chatGPT. Nothing really s...          0\n",
      "\n",
      "[163807 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/kfts06mn6r72x07mq3_05xz80000gn/T/ipykernel_91753/728764588.py:10: FutureWarning: The behavior of obj[i:j] with a float-dtype index is deprecated. In a future version, this will be treated as positional instead of label-based. For label-based slicing, use obj.loc[i:j] instead\n",
      "  df = df[1:]\n"
     ]
    }
   ],
   "source": [
    "# viewing the data\n",
    "data = \"data/chatGPTsentiment/file.csv\"\n",
    "# df = pd.read_csv(data,\n",
    "#                  encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./data/chatGPTsentiment/file.csv\", names=[\"text\",\"sentiment\"])\n",
    "df = df.loc[df[\"sentiment\"] != \"neutral\"]\n",
    "df[\"sentiment\"] = np.where(df[\"sentiment\"]==\"good\", 1,0)\n",
    "df = df[1:]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o12HL7Cs3U75",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Models usually take batches of the same size. It would be useful to check the distribution of the lengths of the sentences in the dataset. (We will need to define a max length of sentences to use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0         Try talking with ChatGPT, our new AI system wh...\n",
       "3.0         THRILLED to share that ChatGPT, our new model ...\n",
       "4.0         As of 2 minutes ago, @OpenAI released their ne...\n",
       "5.0         Just launched ChatGPT, our new AI system which...\n",
       "6.0         As of 2 minutes ago, @OpenAI released their ne...\n",
       "                                  ...                        \n",
       "219287.0    One of my new favorite thing to do with #ChatG...\n",
       "219289.0    Other Software Projects Are Now Trying to Repl...\n",
       "219290.0    I asked #ChatGPT to write a #NYE Joke for SEOs...\n",
       "219291.0    chatgpt is being disassembled until it can onl...\n",
       "219292.0    2023 predictions by #chatGPT. Nothing really s...\n",
       "Name: text, Length: 163807, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Emrbxhyk3U75",
    "outputId": "93e32eae-102e-4f1a-b1e8-e84cbb0f1666",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# TODO: plot the length of the reviews\n",
    "# hint: use the .str.len() method on the first column of the dataframe to get the length of the sentences out\n",
    "# y axis should be the count/number of reviews\n",
    "# x axis should be the length of the reviews\n",
    "# a histogram is a good choice here for visualization\n",
    "# label your axes!\n",
    "\n",
    "def display_histogram(data, title=\"data\"):\n",
    "    lengths = [len(review) for review in data.loc[:,\"text\"]]\n",
    "\n",
    "    x = [length for length in lengths]\n",
    "\n",
    "    plt.hist(x,bins=30)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Tweet Length\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "display_histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmigAhUuC4Yv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO:** Explore the other two data files, similarly to what we have done above (and what we've done on our homeworks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrhH5gxhDGWN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TASK 3: Setting BERT up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgyFw7zkDIM_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__IMPORTANT__:\n",
    "BERT uses two special tokens while tokenization, they are [CLS] and [SEP]. The [CLS] token is used to capture the overall representation of the input sequence for classification tasks, while the [SEP] token is used to indicate sentence boundaries and separate different segments of text, enabling BERT to understand relationships between sentences. These tokens play a vital role in enabling BERT to handle a wide range of natural language processing tasks effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cstsfT2B3U75",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us now begin by understanding the Tokenizer used by BERT and the BERT Model Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bdagkfGR3U76",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import specific models\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# define a max length constant\n",
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8NP_1_q3U76",
    "outputId": "f28430e9-4a8f-4851-ed36-366e427f5888",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 13:52:28.147835: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-12-06 13:52:28.147954: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-12-06 13:52:28.147972: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-12-06 13:52:28.148530: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-06 13:52:28.149195: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# downloads a bunch of stuff. On Felix's computer (in Meserve Hall), this took ~half a minute\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# expect to see:\n",
    "# Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
    "# - This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
    "# - This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "# All the weights of TFBertModel were initialized from the PyTorch model.\n",
    "# If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyC5gU3v-YyG",
    "outputId": "4a033b65-efcf-48ec-f8b4-d59f89b9eb51",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109482240 (417.64 MB)\n",
      "Trainable params: 109482240 (417.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO: print out a summary of your downloaded bert model\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCz4eg5x-bal",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice how there are 109M parameters. These parameters have already been trained. We want to make sure these are not trained when we add on our own components later. (We'll \"freeze\" them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04eD8Fa42cVy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. What kind of text data do these weights encode? (Scour the internet and describe the contents of the data used to train BERT)\n",
    "__BERT was trained on a large corpus of books and wikipedia entries. We might expect these weights to encode information pertaining to events and facts, as well a general structure of patterns found in literature.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmIp4Rs7DVUE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ze5NIO3x3U76",
    "outputId": "d8c5309a-0dac-4c44-f711-645d6ddc1d78",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length is :  30522\n",
      "n e t w o r k\n",
      "[ S E P ]\n",
      "[ M A S K ]\n",
      "12947\n"
     ]
    }
   ],
   "source": [
    "# TODO: print out the first 10 tokens in the tokenizer's vocabulary, the\n",
    "# last ten tokens, and ten tokens from somewhere in the middle of the vocabulary\n",
    "# print out the things necessary to answer the questions below\n",
    "\n",
    "# tokenizer.vocab will access the vocabulary\n",
    "# tokenizer.vocab.keys() will give you the words in the vocabulary. You can convert this to a list to index into it.\n",
    "# tokenizer.vocab[string] will give you the token id for the string\n",
    "# tokenizer.decode(id) will give you the string for the token id\n",
    "\n",
    "print(\"Vocab length is : \",len(tokenizer.vocab))\n",
    "\n",
    "print(tokenizer.decode(2897))\n",
    "print(tokenizer.decode(102))\n",
    "print(tokenizer.decode(103))\n",
    "print(tokenizer.vocab[\"josef\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. What is the token associated with id 2897? __network__\n",
    "3. What is the token associated with id 102? __\\[SEP\\]__\n",
    "4. What is the token associated with id 103? __\\[MASK\\]__\n",
    "5. Is your name in BERT's vocabulary (make sure to used the lowercase version, e.g. \"felix\")? If yes, what is it's id?  __Yes, 12947__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahvhFZSE3U76",
    "outputId": "191e1cad-639f-4d1f-8fad-87b061724857",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  tf.Tensor([[  101  7592  1010  2026  3899  2003 10140  1010  2053  2428   102]], shape=(1, 11), dtype=int32)\n",
      "token_type_ids :  tf.Tensor([[0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 11), dtype=int32)\n",
      "attention_mask :  tf.Tensor([[1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 11), dtype=int32)\n",
      "[CLS] hello, my dog is cute, no really [SEP]\n"
     ]
    }
   ],
   "source": [
    "# example of tokenizing a sentence with the bert tokenizer\n",
    "input_text = \"Hello, my dog is cute, no really\"\n",
    "out = tokenizer(input_text,return_tensors=\"tf\")\n",
    "for key in out:\n",
    "    print(key,\": \",out[key])\n",
    "\n",
    "# notice the differences between the input_text and what is printed out here\n",
    "print(tokenizer.decode(out[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7aibPRm3U76",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice the different keys returned by the tokenizer. The model can take as input all three keys or just one of them the \"input_ids\".\n",
    "Since the model requires a fixed length input, we will need to pad the sequences to the same length. Let us pad and turn the sequences into length 15 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rQN3WHF3U77",
    "outputId": "6d87b813-4038-4284-a770-195a7d91f0a0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  tf.Tensor([[  101  7592  1010  2026  3899  2003  2025 10140  1010  2053  2428   102]], shape=(1, 12), dtype=int32)\n",
      "token_type_ids :  tf.Tensor([[0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 12), dtype=int32)\n",
      "attention_mask :  tf.Tensor([[1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 12), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# TODO: set the max_length and truncation parameters of the tokenizer\n",
    "# the behavior that you want to end up with is a tokenizer that produces tensors that \n",
    "# are never over 15. You are not providing paired input\n",
    "\n",
    "# Tokenizer documentation: https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "\n",
    "input_text = \"Hello, my dog is not cute, no really\"\n",
    "\n",
    "# TODO: update this line\n",
    "out = tokenizer(input_text,return_tensors=\"tf\", max_length=15, truncation=True)\n",
    "\n",
    "\n",
    "for key in out:\n",
    "    print(key,\": \",out[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb7Jhkmh3U77",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The tokenzier also returns multiple outputs for multiple sentences. This is useful for batching.\n",
    "\n",
    "NOTE : The max length is 512 for BERT. We will use 128 or 256 for this lab (our `MAX_LENGTH` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VybL0aDS3U77",
    "outputId": "8175cd1b-c8fc-4522-dea0-bceeb1a62f4a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  (3, 128)\n",
      "token_type_ids :  (3, 128)\n",
      "attention_mask :  (3, 128)\n"
     ]
    }
   ],
   "source": [
    "# TODO: make a list of sentences that you want to tokenize with at least 3 sentences that are padded to MAX_LENGTH\n",
    "# make sure that you can tokenize them all at once by looking at the shapes of the tensors that have been output\n",
    "sents = [\"This is a sentence.\",\"This is another sentence, or is it?\", \"Here is one last sentence.\"]\n",
    "batch_x = tokenizer(sents,return_tensors=\"tf\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# here's some code to look at the shapes of the tensors that have been output\n",
    "for key in batch_x:\n",
    "    print(key,\": \",batch_x[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqoZW31r3U77",
    "outputId": "71082962-4f64-4dfd-d091-7c3a448637d0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run bert on the input_ids\n",
    "out = bert(batch_x[\"input_ids\"])\n",
    "\n",
    "# display what the keys are for us\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auI7ybZq3U77",
    "outputId": "b7ae6ed0-5cbd-40fd-c49e-5f080c09eb82",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 128, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the shape of the last_hidden_state\n",
    "out['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZIJGjHL3U77",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model returns two tensors of interest. One is the pooled output and the other is contextual hidden state over every token. We will use the hidden state associated with the first token [CLS] for classification. Additional strategies involve averaging the non padding states. The pooler output is a summary of the hidden states.\n",
    "\n",
    "5. Investigate the shape of `last_hidden_state`. What is the __meaning__ of each dimension? (why are each dimension numbered as they are/what do they correspond to? You may need to go investigate some documentation to fully answer this!) __The first dimension is the number of sentences in the tokenized batch, which is 3 in this case. The second is the padded length of the sentences, which we have set to 256. The thrid and final dimension is the the number of hidden units in the final encoder, which is 768 for the base model.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-nEKeH13U77",
    "outputId": "21c36e0f-0e09-4591-e33b-d97b1a00772b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First token [CLS] :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n",
       "array([[-0.56981224,  0.37906495,  0.3972259 , ..., -0.61657137,\n",
       "         0.45286912, -0.5461028 ],\n",
       "       [-0.45902807,  0.16471946,  0.5013481 , ..., -0.44628778,\n",
       "         0.36878574, -0.7651645 ],\n",
       "       [-0.60922253,  0.18969248,  0.4445212 , ..., -0.5728813 ,\n",
       "         0.49445856, -0.7689513 ]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also run bert with unpacked inputs\n",
    "out = bert(**batch_x)\n",
    "\n",
    "# or with them as \"input_ids\" explicitly\n",
    "out = bert(input_ids=batch_x[\"input_ids\"])\n",
    "\n",
    "print(\"First token [CLS] :\")\n",
    "out[\"last_hidden_state\"][:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDOKY_s6B8Ie",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Op6JBjoM3U78",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a batch size for our experiments\n",
    "BATCH_SIZE = 2\n",
    "# define a percentage of the data to use for training\n",
    "SPLIT_PC = .80\n",
    "\n",
    "# TODO: caluculate the last index for the training data\n",
    "END = int(len(df.index) * SPLIT_PC) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98309, 2)\n",
      "(65499, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/kfts06mn6r72x07mq3_05xz80000gn/T/ipykernel_91753/951028706.py:2: FutureWarning: The behavior of obj[i:j] with a float-dtype index is deprecated. In a future version, this will be treated as positional instead of label-based. For label-based slicing, use obj.loc[i:j] instead\n",
      "  training = df[:END+1]\n",
      "/var/folders/4d/kfts06mn6r72x07mq3_05xz80000gn/T/ipykernel_91753/951028706.py:3: FutureWarning: The behavior of obj[i:j] with a float-dtype index is deprecated. In a future version, this will be treated as positional instead of label-based. For label-based slicing, use obj.loc[i:j] instead\n",
      "  testing = df[END+1:].reset_index()\n"
     ]
    }
   ],
   "source": [
    "#  TODO: get the training data and testing data set up\n",
    "training = df[:END+1]\n",
    "testing = df[END+1:].reset_index()\n",
    "\n",
    "# TODO: add print statements to verify the sizes of your training/testing data are correct\n",
    "print(training.shape)\n",
    "print(testing.shape)\n",
    "\n",
    "train_sentences = training.loc[:,\"text\"]\n",
    "train_labels = training.loc[:,\"sentiment\"]\n",
    "test_sentences = testing.loc[:,\"text\"]\n",
    "test_labels = testing.loc[:,\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBGgSLsK3U78",
    "outputId": "947b8caa-256c-4934-d11c-f3c5948fbc49",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data generator for the model\n",
    "def data_generator(sentences: np.array,labels: np.array,batch_size: int) -> (dict,tf.Tensor):\n",
    "    i = 0\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        # TODO: append batch_size number of sentences and labels to batch_x and batch_y\n",
    "        # Make sure that you don't re-use sentences and labels that you've already put into batches!\n",
    "        for j in range(batch_size):\n",
    "            START = i * batch_size\n",
    "            batch_x.append(sentences[START+j])\n",
    "            batch_y.append(labels[START+j])\n",
    "        i+=1\n",
    "\n",
    "        if i == len(train_sentences)//BATCH_SIZE:\n",
    "            i = 0\n",
    "\n",
    "        # TODO: tokenize the batch_x, padding to MAX_LENGTH, and truncating to MAX_LENGTH\n",
    "        batch_x = tokenizer(batch_x,return_tensors=\"tf\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "        # debugging prints (make sure that these are commented out when you actually train your model)\n",
    "        # should be (batch_size, MAX_LENGTH)\n",
    "        # print(batch_x['input_ids'].shape)\n",
    "\n",
    "        # convert our ys into the appropriate tensor\n",
    "        batch_y = tf.convert_to_tensor(batch_y)\n",
    "        \n",
    "        # debugging prints (make sure that these are commented out when you actually train your model)\n",
    "        # should be (batch_size,)\n",
    "        # print(batch_y.shape)\n",
    "        yield dict(batch_x), batch_y\n",
    "\n",
    "train_data = data_generator(train_sentences,train_labels,BATCH_SIZE)\n",
    "test_data = data_generator(test_sentences,test_labels,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xwi_HtRmCfFs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**\"Bonus\" Question** - How would you implement randomness in the generator above. (HINT `numpy.random.choice` and its `replace` option)\n",
    "\n",
    "(This is bonus in the sense that it is a good exercise to think about, not in the sense of us giving you extra credit. Feel free to talk to Prof. Felix more about the intersection of equity and extra credit.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "QxuNw_DKBN4T",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:1698\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:1722\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0.0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Take a look at the contents of tmp_batch_x and tmp_batch_y and report the shapes of the `input_ids` \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# and the y label tensor.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# make sure that the shapes are what you expect them to be\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# (take a look at the comments in the data_generator code)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tmp_batch_x,tmp_batch_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(tmp_batch_x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(tmp_batch_y\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m, in \u001b[0;36mdata_generator\u001b[0;34m(sentences, labels, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     10\u001b[0m     START \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m---> 11\u001b[0m     batch_x\u001b[38;5;241m.\u001b[39mappend(\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSTART\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     12\u001b[0m     batch_y\u001b[38;5;241m.\u001b[39mappend(labels[START\u001b[38;5;241m+\u001b[39mj])\n\u001b[1;32m     13\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m   1042\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# TODO: Take a look at the contents of tmp_batch_x and tmp_batch_y and report the shapes of the `input_ids` \n",
    "# and the y label tensor.\n",
    "# make sure that the shapes are what you expect them to be\n",
    "# (take a look at the comments in the data_generator code)\n",
    "\n",
    "tmp_batch_x,tmp_batch_y = next(train_data)\n",
    "\n",
    "print(tmp_batch_x['input_ids'].shape)\n",
    "print(tmp_batch_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n78JGhebB4C_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TASK 4: Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DQjpBh_-liQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remember how you used concatenated embeddings for your NN model for HW5. The model we design in this lab will take as input token_ids (batch_size, embedding_dim) , run it through a non-trainable bert, extract the 768 dim vector associated with the [CLS] token of each sentence in a batch, this 768 dim vector will play the role of our concatenated embeddings. The main take away is this : Any input size up to 512 will return a 768 dim vector we can use as an embedding for the entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9T-oAymn3U78",
    "outputId": "b8f8949d-232a-41d0-ae28-6994008f7e85",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "# This takes < 15 sec to run on Felix's computer\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased',output_attentions = False,return_dict=False)\n",
    "# we do not need attention outputs\n",
    "# we want to return tuples since they are easier to access\n",
    "\n",
    "bert_model.trainable = False\n",
    "# setting trainable to false ensures\n",
    "# we do not update its weights\n",
    "model_ = tf.keras.Sequential([\n",
    "    bert_model,\n",
    "    tf.keras.layers.Lambda(lambda x: x[0][:,0,:]), # https://keras.io/api/layers/core_layers/lambda/\n",
    "    tf.keras.layers.Dense(50,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. What does the `Lambda` layer do? Why do we need it? (Read the documentation and investigate. Think carefully about what `x[0][:,0,:]` means. `x[0]` here is a numpy array.) __It applies the defined lambda function to the input. x\\[0\\] is the last_hidden_state tensor that the model outputs. x\\[0\\]\\[:,0,:\\] returns the portion of the hidden state that is associated with the \\[CLS\\] token.__\n",
    "2. What weights will you be training in this model? __The weights in the sequential model we define in keras__\n",
    "2. What weights will you __not__ be training in this model? __The weights in the BERT model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heoBiDyaBl72",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We use `.fit` here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d_b3IWY_ZZz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will also be adding a validation data generator and validation steps. \n",
    "This will allow us to check accuracy on the test_data wile we train over each epoch.\n",
    "\n",
    "For this part, you'll be training in some different configurations and __recording your results__. (don't forget to write these down!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1Seudgl3U79",
    "outputId": "029c9b07-d7b3-4e07-cb4b-68824b2c285f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "400/400 [==============================] - 174s 409ms/step - loss: 0.4458 - accuracy: 0.7962 - val_loss: 0.2051 - val_accuracy: 0.9375\n",
      "Epoch 2/3\n",
      "400/400 [==============================] - 154s 384ms/step - loss: 0.2985 - accuracy: 0.8825 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "400/400 [==============================] - 154s 384ms/step - loss: 0.2916 - accuracy: 0.8838 - val_loss: 0.0777 - val_accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cbe95f4820>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 epochs takes ~2 and a half minutes on Felix's computer\n",
    "model_.fit(\n",
    "    train_data,\n",
    "    epochs=3,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=len(train_sentences)//BATCH_SIZE,\n",
    "    validation_data=test_data,\n",
    "    validation_steps=BATCH_SIZE*4,\n",
    "    validation_batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of1cKkNACWjH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO: Now train the model on the other two files present in the data folder and report your results (make sure that you train your model from scratch). You will want to experiment with different `MAX_LENGTH`s, `batch_size`s, the number of dense layers you have, the number of hidden units per layer.\n",
    "\n",
    "In general, the more layers corresponds to the more levels of abstract information that your model will be able to extract/represent. The more hidden units (the \"wider\") your network has, the more information it will be able to memorize.\n",
    "\n",
    "4. Using the \"base\" model settings (the default values we've set for you), record the `val_accuracy` after 2 epochs for each training data set:\n",
    "    1. imdb: __0.9062__\n",
    "    2. amazon: __0.8750__\n",
    "    3. yelp: __0.8906__\n",
    "\n",
    "5. Choose one dataset. For at least 4 different combinations of settings, record the following five pieces of information:\n",
    "    1. training data (imdb, amazon, yelp) (this will be the same for all 4 combinations)\n",
    "    2. epochs\n",
    "    3. dimensions of your network (# of layers, # of hidden units per layer) — this is the part after the bert model\n",
    "    4. `MAX_LENGTH` value\n",
    "    5. `batch_size` value\n",
    "    6. time to train\n",
    "    7. `val_accuracy` in the end\n",
    "\n",
    "__YOUR RESULTS HERE (format these nicely!)__\n",
    "\n",
    "| data | epochs | # of layers | # of hidden units | MAX_LENGTH | batch_size | time to train | val_accuracy |\n",
    "|------|--------|-------------|-------------------|------------|------------|---------------|--------------|\n",
    "| imdb | 1      | 1           | 50                | 256        | 4          | 363s          | 0.8906       |\n",
    "| imdb | 2      | 1           | 100               | 256        | 4          | 716s          | 0.9531       |\n",
    "| imdb | 2      | 2           | 25                | 128        | 2          | 303s          | 0.8750       |\n",
    "| imdb | 3      | 2           | 50                | 128        | 2          | 482s          | 0.9375       |\n",
    "\n",
    "\n",
    "6. What did your experiments show you? __Increasing the layers lead to relatively simular performance while using fewer hidden units in total. In general, increasing the number of epochs also helped performance.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
